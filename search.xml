<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome to RJ-Gu&#39;s Homepage!</title>
    <url>/uncategorized/About-this-Homepage/</url>
    <content><![CDATA[<p><strong>Welcome to RJ-Gu’s homepage!</strong> I’m Rongjian Gu, a college student in <a href="https://www.ustc.edu.cn/">USTC</a>. This is a simple blog of mine, where I record some ideas and experiences. </p>
<p>On the left are shortcuts you may find helpful.</p>
<ul>
<li><strong>About</strong> is a short introduction of me, <del>and CV will be accessible to be downloaded someday.</del> Now CV can be downloaded!</li>
<li>You can find essays and articles within <strong>Archives</strong> and <strong>Categories</strong>. Articles are sorted in order of time in <strong>Archives</strong>, while in <strong>Categories</strong> they are categorized.</li>
</ul>
<p>This blog is constructed with <a href="https://hexo.io/">Hexo</a> and <a href="https://github.com/theme-next/hexo-theme-next">NexT</a>. </p>
<p>Please have fun here!</p>
]]></content>
  </entry>
  <entry>
    <title>LLM与检索、记忆相关文章分析</title>
    <url>/LLM-application/LLM-memory-survey/</url>
    <content><![CDATA[<p>因为AI Agents需要处理各种各样的情况，LLM就会作为Agents的一种重要的底层模型。在这个框架下，LLM需要得到多方面的提升才能在Agents中发挥作用。记忆就是其中重要的一部分，Agents需要特定的记忆机制来确保其能够正确的处理复杂的任务，应用之前的策略。此外，这样也能帮助Agents适应陌生的环境。</p>
<span id="more"></span>

<p>LLM中实现记忆存在难点：</p>
<ol>
<li>LLM是以自然语言格式处理交互的；随着总记录的变多，输入数据可能会超过Transformer体系结构的限制，导致之前的数据出现丢失的情况。</li>
<li>LLM很难提取之前获得的记忆。历史记录变多后，Agents的记忆压力增加，在相关的topic之间建立连接的难度变大。</li>
</ol>
<p>有如下帮助Agents增强记忆的方式：</p>
<ol>
<li>提升Transformer模型的长度限制：可以使用文本截断、分割输入、强调关键部分等方法；还可以修改注意力机制从而适应更长的序列。</li>
<li>记忆总结：将记忆浓缩总结。</li>
<li>使用向量或数据结构压缩记忆：可以使用embedding vector&#x2F;triplet等方法存储。</li>
</ol>
<p>记忆的检索是自动的。模型会给不同的记忆打分，分数较高的会优先选择。</p>
<h2 id="Forward-Looking-Active-REtrieval-augmented-generation-FLARE"><a href="#Forward-Looking-Active-REtrieval-augmented-generation-FLARE" class="headerlink" title="Forward-Looking Active REtrieval augmented generation(FLARE)"></a>Forward-Looking Active REtrieval augmented generation(FLARE)</h2><p>文章名：<strong>Active Retrieval Augmented Generation</strong>，LTI@CMU组实现</p>
<p>前瞻性主动检索增强生成</p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>原有的LLM+retrieval主要使用的是一次查询+生成，即：<br>$$<br>生成结果y&#x3D;LM([查询结果\mathcal D_x, \text{query}\ x])<br>$$<br>在Active Retrieval Augmented Generation中需要进行多次查询+生成：<br>$$<br>查询使用的\text{query}\ q_t&#x3D;\text{qry}(x,[y_0,\dots,y_{t-1}])\<br>生成结果y_t&#x3D;LM([\mathcal D_{q_t},x,[y_0,\dots,y_{t_1}]])<br>$$<br>FLARE的intuition：</p>
<ol>
<li>LLM应该只在知识不足的时候进行查询，而非所有时候都进行不必要的查询；</li>
<li>查询的query应该包含未来生成目标的意图。</li>
</ol>
<h4 id="FLARE-instruct"><a href="#FLARE-instruct" class="headerlink" title="FLARE instruct"></a>FLARE instruct</h4><p>eg：<img src="/LLM-application/LLM-memory-survey/image-20240218171820374.png" alt="image-20240218171820374" style="zoom:50%;"></p>
<p>对一些不确定的信息进行查询，并插入到生成的回答中。</p>
<h4 id="FLARE-direct"><a href="#FLARE-direct" class="headerlink" title="FLARE direct"></a>FLARE direct</h4><p>在上述的多次查询+生成中进行修改：</p>
<p>在每一次迭代中：</p>
<ul>
<li><p>先根据query和已经生成的语句生成下一句$\hat{s}_t$，作为备选项；</p>
</li>
<li><p>当LM对$\hat{s}_t$有信心（所有token概率大于一个阈值）时，就直接使用该语句；</p>
</li>
<li><p>否则，仍然类似Active Retrieval Augmented Generation中生成对应结果$y_t&#x3D;LM([\mathcal D_{q_t},x,[y_0,\dots,y_{t_1}]])$</p>
</li>
</ul>
<p>query $q_t$的选择：可以直接使用上一次生成的$\hat s_t$作为$q_t$，效果也很好。但是可能会出现生成的语句出错，导致查询也出错的情况。可以用以下方法解决：</p>
<ul>
<li>将不准确的信息mask掉。</li>
<li>对于不准确的信息，使用LM生成一系列问题，使得答案应为这些不准确的信息。</li>
</ul>
<h2 id="In-Context-Retrieval-Augmented-Language-Models-In-Context-RALM"><a href="#In-Context-Retrieval-Augmented-Language-Models-In-Context-RALM" class="headerlink" title="In-Context Retrieval-Augmented Language Models(In-Context RALM)"></a>In-Context Retrieval-Augmented Language Models(In-Context RALM)</h2><p>文章名：In-Context Retrieval-Augmented Language Models，AI21 Labs实现</p>
<p>就是FLARE中提到的在query前增加retrieval得到的信息。FLARE中说明：该方法可能会导致模型不清楚接下来生成的内容应该与什么方向有关。</p>
<h2 id="Survey-Retrieval-Augmented-Generation-for-LLM"><a href="#Survey-Retrieval-Augmented-Generation-for-LLM" class="headerlink" title="Survey: Retrieval-Augmented Generation for LLM"></a>Survey: Retrieval-Augmented Generation for LLM</h2><p>RAG可分为三类：</p>
<ol>
<li>Naive RAG</li>
<li>Advanced RAG</li>
<li>Modular RAG</li>
</ol>
<h3 id="架构变化"><a href="#架构变化" class="headerlink" title="架构变化"></a>架构变化</h3><h4 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h4><p>遵循检索-读取框架：</p>
<ul>
<li>首先离线搜索相关资料，将pdf等文件转换为纯文本格式；</li>
<li>依照传统的搜索方法（余弦相似度等）找到最相关的K个文档，作为上下文生成的基础；</li>
<li>将上下文合并，作为prompt</li>
</ul>
<p>问题：</p>
<ul>
<li>检索可能出现的问题：精度低、召回率低、信息过时等。</li>
<li>生成可能出现的问题：幻觉（hallucination）等。</li>
<li>增强可能出现的问题：由于搜索结果重复导致的响应重复、输出脱节&#x2F;不连贯等。</li>
<li>需要平衡不同段落的价值；</li>
<li>协调写作风格和预期的差异，以保证输出的一致性；</li>
<li>模型可能过度依赖检索到的内容，而不提供新的信息。</li>
</ul>
<h4 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h4><p>针对性增强Naive RAG的功能。</p>
<p>预检索过程的优化：优化数据索引，提高索引内容的质量。可通过增强数据粒度、优化索引结构、添加源数据、对齐优化和混合检索实现。</p>
<p>检索过程的优化：对嵌入模型（embedding model）进行优化。例：BGE，最新的BGE-M3</p>
<p>检索后过程的优化：对检索到的内容进行额外处理。可以进行重排序、prompt压缩等处理。</p>
<h4 id="Modular-RAG"><a href="#Modular-RAG" class="headerlink" title="Modular RAG"></a>Modular RAG</h4><p>将不同模块合并，形成序列化pipe解决特定问题。</p>
<p>常见模块：搜索模块、记忆模块、路由模块（用于将不同来源的检索结果进行替换、合并）、预测模块（使用LLM直接生成上下文信息，解决噪音问题）、任务适配器（用于使RAG适配下游任务）。</p>
<p>不同模块之间可以进行排列组合以提高灵活性。</p>
<p>例：RRR模型引入了Rewrite-RetrieveRead过程，提高重写部分的性能等。</p>
<p>检索过程优化：可使用混合搜索、递归检索（见Active Retrieval Augmented Generation中的多次检索+查询）、StepBack-prompt（reasoning的方法）、子查询等。</p>
<p><img src="/LLM-application/LLM-memory-survey/image-20240225212201004.png" alt="image-20240225212201004"></p>
<h3 id="检索优化"><a href="#检索优化" class="headerlink" title="检索优化"></a>检索优化</h3><p>解决三个问题：如何实现准确的语义表示？如何将查询和文档的语义空间对齐？如何将输出与LLM的偏好对齐？</p>
<h4 id="语义表示优化"><a href="#语义表示优化" class="headerlink" title="语义表示优化"></a>语义表示优化</h4><ol>
<li>块优化：文档需要分块才能提取细粒度的特征，但块的大小会影响结果。用户查询的query长度和不同的嵌入模型对不同的块大小性能也不同。-&gt; 不存在最佳策略，应该灵活应用。可以使用滑动窗口技术，也可以使用先小后大的方式进行处理。</li>
<li>对嵌入模型做微调：嵌入模型对专业领域能力有限，针对不同的下游任务有问题。针对这两个方面作微调可以有效提升性能。</li>
</ol>
<h4 id="查询与文档对齐"><a href="#查询与文档对齐" class="headerlink" title="查询与文档对齐"></a>查询与文档对齐</h4><p>查询重写，嵌入转换</p>
<h4 id="检索器与LLM对齐"><a href="#检索器与LLM对齐" class="headerlink" title="检索器与LLM对齐"></a>检索器与LLM对齐</h4><p>使用LLM的反馈信号完善检索模型；</p>
<h3 id="生成优化"><a href="#生成优化" class="headerlink" title="生成优化"></a>生成优化</h3><p>对检索出来的文档需要再次进行处理，包括信息压缩、结果重排序。</p>
]]></content>
      <categories>
        <category>LLM application</category>
      </categories>
  </entry>
  <entry>
    <title>LLM对推荐系统的影响调研笔记</title>
    <url>/Essay-analysis/RS-LLM-benefit/</url>
    <content><![CDATA[<p>文章地址：<a href="https://arxiv.org/pdf/2306.05817.pdf">https://arxiv.org/pdf/2306.05817.pdf</a></p>
<p>文章名：How Can Recommender Systems Benefit from Large Language Models: A Survey</p>
<span id="more"></span>

<blockquote>
<p>推荐系统的流水过程：</p>
<ul>
<li><p>用户数据收集<em>User data collection</em></p>
</li>
<li><p>特征处理<em>Feature engineering</em></p>
<p>将原数据进行处理（如独热码）</p>
</li>
<li><p>特征编码<em>Feature encoder</em></p>
<p>生成神经网络的嵌入向量</p>
</li>
<li><p>评分&#x2F;排序函数<em>Scoring Ranking function</em></p>
</li>
<li><p>控制器<em>Pipeline controller</em></p>
</li>
</ul>
</blockquote>
<p>LLM可用于上述过程中除了用户数据收集的部分。</p>
<p>对于特征编码部分，使用LLM可以丰富语义信息，还可实现跨领域推荐。</p>
<p>对于评分&#x2F;排序函数，LLM可用于解决（1）项目评分任务，（2）项目生成任务，（3）混合任务。项目评分任务主要用于针对目标用户，对每个候选项目进行评分，从而获得一个排名列表；项目生成任务直接生成项目的最终排序列表；混合任务上述二者都支持。</p>
<p>早期的语言模型参数比较小，被用作简单的文本特征编码器等；但随着模型和参数量的增多，LLM出现了指令跟随、推理等能力，因此可被扩展用于推荐系统的其他方面。</p>
<p>LLM应用于RS中呈现如下趋势：</p>
<p><img src="/Essay-analysis/RS-LLM-benefit/image-20230728000320090.png" alt="image-20230728000320090"></p>
<p>从微调+与CRM（传统推荐模型）结合（如BERT+推荐模型）到不微调+不与CRM结合（如ChatGPT），再到微调+不与CRM结合&#x2F;不微调+与CRM结合。整体趋势如上图中的箭头所示。</p>
<p>在上述发展过程中，不微调+不与CRM结合的模型效果较差（绿色），但通过增加领域内的协同知识可以提高模型的性能。</p>
]]></content>
      <categories>
        <category>Essay analysis</category>
      </categories>
  </entry>
  <entry>
    <title>Ray部署与测试文档</title>
    <url>/Linux/Ray-deployment/</url>
    <content><![CDATA[<p>本文档为OSH-2023 My-Glow小组的Lab4文档。</p>
<span id="more"></span>
<h1 id="Ray-部署文档"><a href="#Ray-部署文档" class="headerlink" title="Ray 部署文档"></a>Ray 部署文档</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><ul>
<li>Ubuntu版本：22.04.2</li>
<li>python版本：3.10.10</li>
<li>pip版本：23.0.1</li>
</ul>
<h2 id="单机版Ray部署"><a href="#单机版Ray部署" class="headerlink" title="单机版Ray部署"></a>单机版Ray部署</h2><h3 id="安装Ray"><a href="#安装Ray" class="headerlink" title="安装Ray"></a>安装Ray</h3><p>Ray的安装基于pip。运行以下命令安装Ray：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo pip3 install -U <span class="string">&quot;ray[default]&quot;</span></span><br></pre></td></tr></table></figure>

<p>自动安装了ray2.3.1版本。</p>
<blockquote>
<p>根据官方文档，运行<code>sudo pip3 install -U ray</code>也可；与安装<code>ray[default]</code>的区别在于<code>ray[default]</code>会多安装一些依赖。</p>
</blockquote>
<h3 id="Ray-cluster部署"><a href="#Ray-cluster部署" class="headerlink" title="Ray cluster部署"></a>Ray cluster部署</h3><p>Ray集群的部署要求集群内的机器在同一局域网下，并且python和Ray版本相同。</p>
<ul>
<li><p>在主节点上：运行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ray start --<span class="built_in">head</span> --port=6379</span><br></pre></td></tr></table></figure>

<p>得到如下输出，则Ray启动成功：</p>
<p><img src="/Linux/Ray-deployment/image-20230606154156212.png" alt="image-20230606154156212"></p>
</li>
<li><p>在子结点上：运行如下命令：（该命令即为主节点上运行时输出的命令）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ray start --address=<span class="string">&#x27;198.18.0.1:6379&#x27;</span></span><br></pre></td></tr></table></figure>

<p>得到如下输出，则Ray子结点已连接到主节点上：</p>
<p><img src="/Linux/Ray-deployment/image-20230606154333125.png" alt="image-20230606154333125"></p>
</li>
<li><p>要结束ray，只需在主节点上运行<code>ray stop</code>即可。</p>
</li>
</ul>
<h3 id="Ray监控"><a href="#Ray监控" class="headerlink" title="Ray监控"></a>Ray监控</h3><p>在主节点上登陆<code>localhost:8265</code>即可看到所有节点的监控信息。</p>
<p><img src="/Linux/Ray-deployment/image-20230606160319466.png" alt="image-20230606160319466"></p>
<p>这里使用了旧ui。</p>
<h2 id="docker部署Ray"><a href="#docker部署Ray" class="headerlink" title="docker部署Ray"></a>docker部署Ray</h2><p>首先docker的安装见<a href="https://docs.docker.com/engine/install/ubuntu/">官方文档</a>，换源见<a href="https://mirrors.ustc.edu.cn/help/dockerhub.html">中科大docker镜像</a></p>
<p>Dockerfile如下：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sudo sed -i <span class="string">&#x27;s/cn.archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sudo sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sudo sed -i <span class="string">&#x27;s/security.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sudo apt update</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sudo apt install -y python3 &amp;&amp; sudo apt install -y python3-pip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip3 config ser global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip3 install -U <span class="string">&quot;ray[default]&quot;</span></span></span><br></pre></td></tr></table></figure>

<p>使用该镜像来创建ray镜像。步骤如下：</p>
<ol>
<li><p>在目录下创建Dockerfile，并将测试的python文件复制到同一文件夹下，如下图：</p>
<p><img src="/Linux/Ray-deployment/image-20230606172913907.png" alt="image-20230606172913907"></p>
</li>
<li><p>构建docker镜像：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker build -t ray_cluster .</span><br></pre></td></tr></table></figure>

<blockquote>
<p>-t参数的目的是将打包的镜像文件命名。</p>
</blockquote>
</li>
<li><p>运行docker镜像：</p>
<p>头结点：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -idt --name ray_head ray_cluster</span><br></pre></td></tr></table></figure>

<p>子结点：（此处子结点开启了两个，可按需求开启）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -idt --name ray_node1 ray_cluster</span><br><span class="line">docker run -idt --name ray_node2 ray_cluster</span><br></pre></td></tr></table></figure>

<blockquote>
<p>-d参数的目的是后台执行容器；-it使得容器不会自动退出，并能够通过终端访问。</p>
</blockquote>
</li>
<li><p>进入容器，并开启&#x2F;连接Ray服务</p>
<p>对于头结点：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ray_head bin/bash</span><br><span class="line"><span class="comment"># 以下为容器内部操作</span></span><br><span class="line">ray start --<span class="built_in">head</span> --port=6379</span><br></pre></td></tr></table></figure>

<p>可以得到与单机版部署类似的输出。</p>
<p>对于子结点：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ray_node1 bin/bash</span><br><span class="line"><span class="comment"># 以下为容器内部操作</span></span><br><span class="line"><span class="comment"># address为头结点输出的地址</span></span><br><span class="line">ray start --address=<span class="string">&#x27;172.17.0.2:6379&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在头结点内运行测试文件即可开始测试。</p>
</li>
</ol>
<h1 id="Ray性能测试与优化"><a href="#Ray性能测试与优化" class="headerlink" title="Ray性能测试与优化"></a>Ray性能测试与优化</h1><h2 id="测试任务"><a href="#测试任务" class="headerlink" title="测试任务"></a>测试任务</h2><p>我们组选用的测试任务是计算一定范围内（暴力计算）质数的个数。</p>
<blockquote>
<p>选用该任务的原因是该任务计算量大，能够体现出分布式计算的优点；同时暴力计算便于并行化，程序编写简单，且运行时间适中，避免偶然因素影响实验结果。</p>
</blockquote>
<p>暴力计算素数个数的核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_prime</span>(<span class="params">n: <span class="built_in">int</span></span>):</span><br><span class="line">    result = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">int</span>(n ** <span class="number">0.5</span>) + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> n % k == <span class="number">0</span>:</span><br><span class="line">            result = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_primes</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> is_prime(k):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line">count_primes(<span class="number">10000000</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Ray性能指标"><a href="#Ray性能指标" class="headerlink" title="Ray性能指标"></a>Ray性能指标</h2><p>性能指标如下：</p>
<ul>
<li><p>CPU占用率：指计算任务在CPU上的负载。</p>
<p>CPU占用率代表了任务在不同节点内对CPU的压力。</p>
</li>
<li><p>总执行时间：指同一程序从开始执行到执行完毕的时间。</p>
<p>总执行时间代表了一项任务分配在不同节点后的性能表现。</p>
</li>
<li><p>平均响应时间：指系统对请求做出响应的平均时间</p>
<p>平均响应时间代表了一个系统对大量请求的处理能力；在Ray系统内可指代从程序开始执行到Ray开始真正计算的间隔。</p>
</li>
<li><p>吞吐量：指系统在单位时间内处理请求的数量。</p>
<p>吞吐量代表了一个系统单位时间内能够处理请求的数量。</p>
</li>
<li><p>响应时间方差：指将所有节点的响应时间排成数列，该数列的方差。</p>
<p>响应时间方差从另一方面代表了一个系统对大量请求的处理能力；如果方差过大，则代表该系统分配任务存在不均衡的情况，该系统的性能大概率会比较低。</p>
</li>
</ul>
<p>在本实验中，我们将监测CPU占用率和总执行时间两个指标；原因是这两个指标易于测量：CPU占用率可以从Ray控制台上直接得到；总执行时间可以在程序中计时得到。</p>
<blockquote>
<p>程序计时测量执行时间可能存在系统误差，但拉长执行时间后该系统误差会被稀释，对整体的影响较小，因此可以使用。</p>
</blockquote>
<h2 id="不使用Ray进行测试"><a href="#不使用Ray进行测试" class="headerlink" title="不使用Ray进行测试"></a>不使用Ray进行测试</h2><p>CPU使用率测试结果如下：（使用top监测）</p>
<p><img src="/Linux/Ray-deployment/image-20230607170647250.png" alt="image-20230607170647250"></p>
<p>总执行时间时间测试结果如下：</p>
<img src="/Linux/Ray-deployment/image-20230607080229143.png" alt="image-20230607080229143" style="zoom:67%;">

<p>上述结果汇总为表格如下：</p>
<table>
<thead>
<tr>
<th align="center">运行平台</th>
<th align="center">CPU占用率</th>
<th align="center">执行时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center">不使用Ray</td>
<td align="center">98%+（单核）</td>
<td align="center">62.56s</td>
</tr>
</tbody></table>
<p>该结果将作为使用Ray测试时的性能基准。</p>
<h2 id="Ray单机性能测试"><a href="#Ray单机性能测试" class="headerlink" title="Ray单机性能测试"></a>Ray单机性能测试</h2><h3 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Ray.remote</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Compute</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_prime</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        result = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">int</span>(n ** <span class="number">0.5</span>) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> n % k == <span class="number">0</span>:</span><br><span class="line">                result = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_primes</span>(<span class="params">self, start: <span class="built_in">int</span>, end: <span class="built_in">int</span>, step: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, end, step):</span><br><span class="line">            <span class="keyword">if</span> Compute.is_prime(k):</span><br><span class="line">                self.count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.count</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">result</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.count</span><br><span class="line">   </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    Ray.init()</span><br><span class="line">    init_time = time.time()</span><br><span class="line">    computer = Compute.remote()</span><br><span class="line">    futures = []</span><br><span class="line">    future = computer.count_primes.remote(<span class="number">2</span>, <span class="number">10000000</span>, <span class="number">1</span>)</span><br><span class="line">    futures.append(future)</span><br><span class="line">    result = Ray.get(futures)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time: &quot;</span>, time.time()-init_time)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在一台虚拟机上运行一个头结点和两个子结点，作为Ray集群。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>运行计算质数个数的代码，得到如下结果：</p>
<p><img src="/Linux/Ray-deployment/image-20230607165726373.png" alt="image-20230607165726373"></p>
<p>运行过程中Ray的控制台如下所示：</p>
<p><img src="/Linux/Ray-deployment/image-20230607165619816.png" alt="image-20230607165619816"></p>
<p>观测top内CPU的使用率如下：</p>
<p><img src="/Linux/Ray-deployment/image-20230607171423096.png" alt="image-20230607171423096"></p>
<p>top内显示Ray计算只用了一个进程，显然没有对多核计算进行优化。</p>
<p>总结出单机运行Ray的性能指标如下表所示：</p>
<table>
<thead>
<tr>
<th align="center">运行平台</th>
<th align="center">CPU占用率</th>
<th align="center">执行时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center">不使用Ray</td>
<td align="center">98%+（单核）</td>
<td align="center">62.56s</td>
</tr>
<tr>
<td align="center">使用Ray</td>
<td align="center">&lt;20%（所有核）~100%（单核）</td>
<td align="center">64.32s</td>
</tr>
</tbody></table>
<p>可以看到，程序部署在Ray后，运行时间相比未部署的时间反而长了2s（应为Ray分配任务的开销），且CPU的利用率仅不到20%，说明上述代码并没有利用到Ray分布式集群强大的运算能力，应该进行优化。</p>
<h2 id="Ray单机优化性能测试"><a href="#Ray单机优化性能测试" class="headerlink" title="Ray单机优化性能测试"></a>Ray单机优化性能测试</h2><p>针对上述代码无法利用到Ray性能的问题，考虑将测试代码进行优化。</p>
<p>最初的测试代码只是将任务整体交给Ray，因此用不到Ray的并行计算。因此，在计算任务中，将所有任务手动拆分为小块，并分配给Ray，性能预期将会提升。</p>
<p>修改main函数如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    Ray.init()</span><br><span class="line">    init_time = time.time()</span><br><span class="line">    <span class="comment"># step: 指分组数，每一组内检查的数差值为step</span></span><br><span class="line">    step = <span class="number">10</span></span><br><span class="line">    futures = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step):</span><br><span class="line">        computer = Compute.remote()</span><br><span class="line">        future = computer.count_primes.remote(i, <span class="number">10000000</span>, step)</span><br><span class="line">        futures.append(future)</span><br><span class="line">    results = Ray.get(futures)</span><br><span class="line">    <span class="comment"># 因为计算时把0和1也算入质数，因此在这里减去</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">sum</span>(results) - <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time: &quot;</span>, time.time()-init_time)</span><br></pre></td></tr></table></figure>

<p>代码将待测试的所有数分为10组，并调用多个<code>remote()</code>方法生成多个计算实例并同时计算，从而达到更高的计算性能。</p>
<p>测试结果如下所示：</p>
<p><img src="/Linux/Ray-deployment/image-20230607171912591.png" alt="image-20230607171912591"></p>
<p>运行过程中Ray的控制台如下所示：</p>
<p><img src="/Linux/Ray-deployment/image-20230607171116498.png" alt="image-20230607171116498"></p>
<blockquote>
<p>图中显示第二个节点CPU使用率显著低于其他两个节点，可能原因是其正处于被分配过程中，因此可视为误差。</p>
</blockquote>
<p>观测top内CPU的使用率如下：</p>
<p><img src="/Linux/Ray-deployment/image-20230607171158315.png" alt="image-20230607171158315"></p>
<p>可见Ray自动开启了多个compute进程，提高了运算速度。</p>
<p>由上述所有总结出单机运行Ray的性能指标如下表所示：</p>
<table>
<thead>
<tr>
<th align="center">运行平台</th>
<th align="center">CPU占用率</th>
<th align="center">执行时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center">不使用Ray</td>
<td align="center">98%+（单进程）</td>
<td align="center">62.56s</td>
</tr>
<tr>
<td align="center">使用Ray</td>
<td align="center">&lt;20%（对任一节点）~100%（单进程）</td>
<td align="center">64.32s</td>
</tr>
<tr>
<td align="center">使用Ray（代码已优化）</td>
<td align="center">99%+（对任一节点） ~100%（多进程）</td>
<td align="center">20.82s</td>
</tr>
</tbody></table>
<p>优化过后代码的性能提升约为<strong>3倍</strong>，说明利用到并行的代码得到了质的性能提升。</p>
<h2 id="Ray分布式性能测试"><a href="#Ray分布式性能测试" class="headerlink" title="Ray分布式性能测试"></a>Ray分布式性能测试</h2><p>分布式测试在两台虚拟机上进行。在一台虚拟机上运行Ray的头节点，在另一台虚拟机上运行子结点，从而构成Ray集群。</p>
<p>Ray的控制台如下所示：</p>
<p><img src="/Linux/Ray-deployment/image-20230608224918948.png" alt="image-20230608224918948"></p>
<p>两节点的IP地址不同可以证明Ray集群是在两台机器上搭建的。</p>
<p>在头结点运行测试程序，得到如下结果：</p>
<p><img src="/Linux/Ray-deployment/image-20230608225313887.png" alt="image-20230608225313887"></p>
<p>观察到两台虚拟机的CPU使用率如下，即CPU负荷均达到最大：</p>
<p><img src="/Linux/Ray-deployment/image-20230608225137644.png" alt="image-20230608225137644"></p>
<p><img src="/Linux/Ray-deployment/image-20230608225154986.png" alt="image-20230608225154986"></p>
<p>Ray的控制台输出如下所示：</p>
<p><img src="/Linux/Ray-deployment/image-20230608225254043.png" alt="image-20230608225254043"></p>
<p>由上述测试结果可以得到Ray已经在多机上完成了分布式部署。</p>
<h2 id="Docker部署性能测试"><a href="#Docker部署性能测试" class="headerlink" title="Docker部署性能测试"></a>Docker部署性能测试</h2><p>Docker的部署过程见部署文档。在完成部署文档的内容后，机器中应该已经有Docker的镜像了。</p>
<p>在终端1上运行docker镜像，并在内部的终端中运行<code>ray start --head --port=6379</code>，得到如下输出：</p>
<p><img src="/Linux/Ray-deployment/image-20230609104415409.png" alt="image-20230609104415409"></p>
<p>类似的，在终端2的docker内运行<code>ray start --address=&#39;172.17.0.2:6379&#39;</code>，有以下输出：</p>
<p><img src="/Linux/Ray-deployment/image-20230609104628643.png" alt="image-20230609104628643"></p>
<p>由于docker内无法通过浏览器显示ray控制台，因此在头结点内使用<code>ray status</code>查看集群的状态：</p>
<img src="/Linux/Ray-deployment/image-20230609104738643.png" alt="image-20230609104738643" style="zoom:50%;">

<p>可见在docker搭建的ray集群内有两个节点，即两台终端分别对应的节点。</p>
<p>在头结点内运行测试文件，得到如下输出：</p>
<p><img src="/Linux/Ray-deployment/image-20230609105008578.png" alt="image-20230609105008578"></p>
<p>这个结果与单机上部署多个ray节点测试结果类似，说明在docker上部署是成功的。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Ubuntu虚拟机搭建</title>
    <url>/Linux/Ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>最近vmware workstation出17版本了，加上之前Ubuntu磁盘管理出了一些问题，识别不到虚拟机新分配的硬盘空间，于是打算重装一下Ubuntu，记录一下过程。</p>
<span id="more"></span>

<h2 id="安装VMware-Workstation-17"><a href="#安装VMware-Workstation-17" class="headerlink" title="安装VMware Workstation 17"></a>安装VMware Workstation 17</h2><p>安装包地址：<a href="https://www.vmware.com/go/getworkstation-win">VMware Workstation 17</a></p>
<p>许可证：MC60H-DWHD5-H80U9-6V85M-8280D</p>
<h2 id="安装Ubuntu"><a href="#安装Ubuntu" class="headerlink" title="安装Ubuntu"></a>安装Ubuntu</h2><p>此处使用了Ubuntu的22.04.1版本。镜像位于<a href="https://mirrors.ustc.edu.cn/ubuntu-releases/22.04.1/ubuntu-22.04.1-desktop-amd64.iso">USTC ubuntu镜像</a>。</p>
<p>推荐使用“典型”安装方式。按照个人电脑的配置进行CPU、内存和硬盘的分配；此处我分配了256GB的硬盘和8GB的内存，CPU个数为1，CPU内核个数为16。其余选项默认。此处分配了较大的空间，主要目的是防止以后有磁盘空间不够的情况出现。</p>
<p>Ubuntu出现安装界面时，选择“minimum installation”，避免安装大量的无用软件；取消勾选“download updates”，待安装完成后在进行update。</p>
<blockquote>
<p>不勾选自动更新是因为此时未换源，更新速度很可能极慢。</p>
</blockquote>
<p>安装完成后Ubuntu就可以使用了，但是为了更好地使用，还需要进行一定的配置。</p>
<h2 id="配置Ubuntu"><a href="#配置Ubuntu" class="headerlink" title="配置Ubuntu"></a>配置Ubuntu</h2><h3 id="换软件源-USTC源"><a href="#换软件源-USTC源" class="headerlink" title="换软件源(USTC源)"></a>换软件源(USTC源)</h3><p>打开终端，输入以下命令：</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo sed -i <span class="string">&#x27;s/cn.archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line">sudo sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line">sudo sed -i <span class="string">&#x27;s/security.ubuntu.com/mirrors.ustc.edu.cn/g&#x27;</span> /etc/apt/sources.list</span><br></pre></td></tr></table></figure>

<p>可将软件源更换为ustc源。再打开sources.list，可见所有的archive.ubuntu.com均换成了mirrors.ustc.edu.cn。</p>
<blockquote>
<p>sudo为使用管理员权限执行命令；sed为linux中的一个文本脚本编辑器；后续参数<code>s/a/b/g</code>将所有的a替换为b。</p>
<p>sources.list的位置即为最后的参数<code>etc/apt/sources.list</code>。</p>
</blockquote>
<p>换源之后，apt下载软件包速度会变快，并能防止连接不上服务器的情况发生。</p>
<p>再进行软件源的更新与软件的更新</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade -y</span><br></pre></td></tr></table></figure>

<p>即可自动将软件包更新到最新的版本。</p>
<blockquote>
<p>apt为Ubuntu中的软件包管理器，可从上述修改的软件源中获取软件包，并自动安装。与之对应的，由已经在本地的<code>dpkg</code>文件直接安装的包管理器为dpkg。</p>
<p><code>apt update</code>指更新软件源列表，可以检查本地的软件是否有更新；<code>apt upgrade</code>指更新软件，将<code>apt update</code>获取的可更新列表付诸实践，进行软件的更新。</p>
<p>推荐先进行<code>apt update</code>，再进行<code>apt upgrade</code>。</p>
<p>在<code>apt update</code>的终端输出中，观察到所有的下载地址均为“&#x2F;mirrors.ustc.edu.cn&#x2F;”，也可以证明上述修改软件源的操作是正确的。</p>
</blockquote>
<h3 id="修改语言"><a href="#修改语言" class="headerlink" title="修改语言"></a>修改语言</h3><p>settings &gt;&gt; region &amp; language &gt;&gt; manange installed language &gt;&gt; install&#x2F;remove languages即可。</p>
<p>也可通过终端更改，见<a href="https://blog.csdn.net/BobYuan888/article/details/88662779">Ubuntu修改终端下的语言（中文或英文）</a></p>
<blockquote>
<p>除了更改语言与使用浏览器等需要桌面GUI的操作，其他情况下均推荐使用终端控制。</p>
</blockquote>
<p>现在Ubuntu语言就已经更换为中文了。</p>
<h3 id="安装常用软件"><a href="#安装常用软件" class="headerlink" title="安装常用软件"></a>安装常用软件</h3><p>终端运行如下命令：</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install vim build-essential python3-pip git xxd wget curl net-tools</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>apt install</code>后跟多个参数可以同时安装多个软件包。</p>
<p>以下依次概括常用软件的用途：</p>
<p><code>vim</code>：终端文本编辑器。linux上编辑文件最常用的文本编辑器，所有终端编辑文件的操作基本均需使用。vim的使用方法此处不展开，可自行查询。</p>
<p><code>build-essential</code>：c&#x2F;c++编译器。包括gcc、g++、make等。</p>
<p><code>python3-pip</code>：python的包管理器。安装python库大都需要该程序。（因为有<code>conda</code>）</p>
<p><code>git</code>：版本控制系统。可以进行文本文件的版本控制。git的使用方法此处不展开，可自行查询。</p>
<p><code>xxd</code>：16进制转换器。可将文件以16进制形式存储。</p>
<p><code>wget</code>&#x2F;<code>curl</code>：网络下载器。可从指定的链接下载网络上的文件。同时二者还有测试网络连接等等功能。</p>
<p><code>net-tools</code>：网络工具箱。包括了<code>ifconfig</code>等命令，可用于检测网络状况。</p>
</blockquote>
<h3 id="ssh与远程"><a href="#ssh与远程" class="headerlink" title="ssh与远程"></a>ssh与远程</h3><p>ssh是Secure SHell的缩写。通过ssh可以实现远程访问终端，则以后需要打开终端时就不需要在虚拟机上打开了。如果不需要Ubuntu的桌面，则完全可以只使用ssh控制，将虚拟机放在后台。</p>
<p>打开终端，输入以下命令：</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install openssh-server openssh-client</span><br></pre></td></tr></table></figure>

<p>此时虚拟机上就已经有ssh软件包了。</p>
<blockquote>
<p>openssh-server允许本机作为宿主机，其他机器连接到本机上；</p>
<p>openssh-client允许本机连接其他机器。</p>
</blockquote>
<h4 id="账号密码连接"><a href="#账号密码连接" class="headerlink" title="账号密码连接"></a>账号密码连接</h4><p>在虚拟机的终端输入<code>ifconfig</code>命令，找到<strong>ens33</strong>内<strong>inet</strong>后的ipv4地址，即为主机连接虚拟机的地址。</p>
<p>在主机远程软件（例如xshell、terminus等）上输入该地址，并输入账号密码，即可远程登录该主机。</p>
<p>从此以后，涉及到的所有终端操作均可通过远程连接。</p>
<h4 id="密钥连接"><a href="#密钥连接" class="headerlink" title="密钥连接"></a>密钥连接</h4><p>远程密钥连接需要在本地生成私钥与公钥，将公钥发给外部机器，外部机器即可访问本机。</p>
<p>终端输入以下命令，并连打三次回车以生成密钥：（引号内为邮箱地址）</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;xxx@yyy.com&quot;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>连打三次回车的目的：</p>
<ul>
<li>第一次为输入密钥存放的地址，此处直接选择默认地址即可；</li>
<li>第二次与第三次为输入与验证passphrase，无需要则可以直接回车略过。</li>
</ul>
</blockquote>
<p>此时密钥已经生成在**&#x2F;home&#x2F;usrname&#x2F;.ssh**文件夹内了。</p>
<p>通过sftp连接主机（可使用账号密码连接），将**&#x2F;home&#x2F;usrname&#x2F;.ssh&#x2F;id_rsa**文件复制到宿主机上，并通过该私钥连接虚拟机即可。</p>
<h4 id="git-ssh配置"><a href="#git-ssh配置" class="headerlink" title="git ssh配置"></a>git ssh配置</h4><p>git可以通过上述生成的密钥进行ssh连接，保护了pull、clone、push等操作的安全性。此处不做详解，可在网络上找到教程。</p>
<h3 id="桌面环境配置"><a href="#桌面环境配置" class="headerlink" title="桌面环境配置"></a>桌面环境配置</h3><p>Ubuntu默认的桌面为gnome桌面，修改桌面的外观需要与gnome有关的插件。</p>
<p>终端运行以下指令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install gnome-tweak chrome-gnome-shell gnome-shell-extensions gnome-extensions-app</span><br></pre></td></tr></table></figure>

<p>即可获取与修改gnome桌面有关的依赖软件。</p>
<p>重启机器，使更改生效；在应用程序列表中将找到tweak（优化）与extensions（扩展）。</p>
<p>在优化中：</p>
<ul>
<li>“字体”下的“缩放比例”可以更改桌面的缩放大小；</li>
<li>“键盘与鼠标”下的“其他布局选项”可以修改键盘映射；个人常将<kbd>Esc</kbd>与<kbd>Caps Lock</kbd>互换以便于vim的编辑，则在“Caps Lock 行为”中找到“Swap Esc and Caps Lock”并选中即可。</li>
</ul>
<h3 id="（可选）zsh、oh-my-zsh、p10k及相关插件的安装"><a href="#（可选）zsh、oh-my-zsh、p10k及相关插件的安装" class="headerlink" title="（可选）zsh、oh-my-zsh、p10k及相关插件的安装"></a>（可选）zsh、oh-my-zsh、p10k及相关插件的安装</h3><p>zsh是linux系统中除bash外的另一种shell，功能比bash强大，可添加插件等，对于终端的操作有很大帮助。</p>
<p>oh-my-zsh和powerlevel10k均为zsh的插件，可以美化zsh。</p>
<p><strong>以下软件的安装均为可选项，主要目的在于美化与增强终端功能，使用bash也不会造成过大的影响。</strong></p>
<h4 id="相关字体的安装"><a href="#相关字体的安装" class="headerlink" title="相关字体的安装"></a>相关字体的安装</h4><p>为了正常显示p10k中的字体，首先进行相关字体的安装。</p>
<p>前往<a href="https://github.com/romkatv/powerlevel10k#fonts">Fonts</a>页面，下载其中的**MesloLGS NF ***字体，并安装。此处在桌面操作较方便。</p>
<p>终端输入以下命令，以检验安装是否成功。</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fc-list | grep MesloLGS</span><br></pre></td></tr></table></figure>

<p>如果在列表中能看到各MesloLGS NF字体，则安装成功。</p>
<blockquote>
<p><code>fc-list</code>用于显示所有安装的字体。</p>
<p>由于字体过多，列表将会很长，不利于查找是否有所需的字体。于是使用<code>grep</code>命令，在给定的文本中寻找MesloLGS字体。</p>
<p><code>|</code>是管道 (pipe)，用于将前一指令的输出重定向为后一指令的输入。</p>
</blockquote>
<p>注意应在终端（远程、本地）的配置中修改显示字体为MesloLGS。</p>
<h4 id="zsh安装"><a href="#zsh安装" class="headerlink" title="zsh安装"></a>zsh安装</h4><p>终端输入<code>sudo apt install zsh</code>即可。</p>
<p>zsh的配置文件在<code>~/.zshrc</code>中，可以进行主题更换、插件安装等操作。具体在以下步骤中说明。</p>
<h4 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h4><p>终端输入以下指令：</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>或者使用<code>wget</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>即可安装oh-my-zsh。过程中会询问是否将默认终端转换为zsh，此处输入y选择是。</p>
<blockquote>
<p>此处可以看出<code>wget</code>与<code>curl</code>有相似之处。</p>
</blockquote>
<p>安装完成后发现shell的提示符变为箭头→，说明默认终端已经更改为了zsh。</p>
<h4 id="oh-my-zsh主题更换"><a href="#oh-my-zsh主题更换" class="headerlink" title="oh-my-zsh主题更换"></a>oh-my-zsh主题更换</h4><p>oh-my-zsh已经有很多的主题，在<code>.zshrc</code>中选用合适的主题即可。</p>
<p>例如想要更换为agnoster主题，则只需在<code>.zshrc</code>中修改<code>ZSH_THEME=&quot;agnoster&quot;</code>，再执行<code>source ~/.zshrc</code>重新加载<code>.zshrc</code>即可看到更改后的主题。</p>
<h4 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h4><p>终端输入以下指令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/romkatv/powerlevel10k.git <span class="variable">$&#123;ZSH_CUSTOM:-<span class="variable">$HOME</span>/.oh-my-zsh/custom&#125;</span>/themes/powerlevel10k</span><br></pre></td></tr></table></figure>

<p>即可安装powerlevel10k。</p>
<p>在<code>.zshrc</code>中修改<code>ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;</code>，再执行<code>source ~/.zshrc</code>重新加载<code>.zshrc</code>即可进入powerlevel10k的配置界面。按照提示操作即可配置p10k的界面设置。</p>
<p>若要重新配置p10k，则在终端输入<code>p10k configure</code>即可重新回到配置界面。</p>
<h4 id="zsh插件安装"><a href="#zsh插件安装" class="headerlink" title="zsh插件安装"></a>zsh插件安装</h4><h5 id="zsh-syntax-highlighting"><a href="#zsh-syntax-highlighting" class="headerlink" title="zsh-syntax-highlighting"></a>zsh-syntax-highlighting</h5><p>这个插件可以帮助检查语法。输入正确的语法会显示绿色，错误的会显示红色。</p>
<p>终端输入以下指令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>

<p>修改<code>.zshrc</code>中<code>plugins=(git zsh-syntax-highlighting)</code>，再执行<code>source ~/.zshrc</code>重新加载<code>.zshrc</code>即可。</p>
<h5 id="其他插件"><a href="#其他插件" class="headerlink" title="其他插件"></a>其他插件</h5><p>另有zsh-autosuggestions插件，可用于自动补全之前输入过的命令。安装方法与上述插件类似。地址如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>

<h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>Ubuntu的配置基本到此就结束了。目前已经可以使用Ubuntu进行基本的开发工作了。</p>
<p>Linux内容博大精深，上述安装只是一个了解Linux的入口。深入了解Linux需要大量的实践与试错。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>LLM与个性化技术相关调研</title>
    <url>/LLM-application/LLM-personalization-survey/</url>
    <content><![CDATA[<p>相较于传统的搜索引擎和推荐系统是在被动的筛选信息，LLM能够积极主动的探索用户意图，通过多轮交互来确定用户的喜好和兴趣。</p>
<p>eg. 推荐系统中：基于内容的推荐是无法捕获用户偏好的，可以考虑LLM改进。</p>
<p><img src="/LLM-application/LLM-personalization-survey/image-20240226185625655.png" alt="image-20240226185625655"></p>
<span id="more"></span>

<h3 id="LLM作为知识库"><a href="#LLM作为知识库" class="headerlink" title="LLM作为知识库"></a>LLM作为知识库</h3><p>首先，可以直接通过三元组的头部实体和关系来预测尾部实体。因为LLM中存有大量常识知识，也可以通过LLM构建知识图谱。LLM还可以补全知识图谱，提升原有的知识图谱的质量。</p>
<p>应当注意LLM此时的幻觉问题。</p>
<h3 id="LLM作为内容翻译器-content-interpreter"><a href="#LLM作为内容翻译器-content-interpreter" class="headerlink" title="LLM作为内容翻译器(content interpreter)"></a>LLM作为内容翻译器(content interpreter)</h3><p>LLM可以将输入信息理解后分析出一些内部信息，用于推荐。</p>
<h3 id="LLM作为解释器（explainer）"><a href="#LLM作为解释器（explainer）" class="headerlink" title="LLM作为解释器（explainer）"></a>LLM作为解释器（explainer）</h3><p>LLM可以给出推荐某个物品的理由。通过这种方式可以提高模型的说服力和可靠性，也利于识别纠正错误。</p>
<p>相比于传统方法，LLM实现的可解释推荐能够不受模板约束，更加适合用户偏好的定制解释，减少局限性。此外，它也能获得用户即时反馈，促进双向对齐。</p>
<p>问题：无法找到给出解释的依据；给出的解释可能与推荐的行为不一致；同样的线索可能给出不同的解释。</p>
<h3 id="LLM做推理"><a href="#LLM做推理" class="headerlink" title="LLM做推理"></a>LLM做推理</h3><p>LLM拥有一定的推理能力，能够帮助挖掘用户兴趣。</p>
<p>LLM可以不做调整，直接进行推理推荐。但是存在问题：</p>
<ol>
<li>在公开数据集上表现好，但私有数据集上表现差(zero shot)；</li>
<li>LLM与传统推荐模型的推荐性能拉不开差距。</li>
</ol>
<p>此外，LLM还可以与其他算法结合（如遗传算法，LLM生成解空间中的可选解），从而获得比直接推理更好的性能，还能维持一定的可解释性。</p>
<p>存在的问题：复杂度比较高 等问题。</p>
<h3 id="LLM作为对话代理"><a href="#LLM作为对话代理" class="headerlink" title="LLM作为对话代理"></a>LLM作为对话代理</h3><p>会话式推荐系统可以实时理解用户意图，根据反馈调整推荐。</p>
<p>LLM可以通过一系列QA获取用户偏好来进行推荐。同样的，存在一系列问题：针对私有数据的效果差，可通过微调解决；prompt仍然需要引导才能成功推理；长时间的记忆存在问题（retrieval）。</p>
<h3 id="LLM用于工具学习-tool-learning"><a href="#LLM用于工具学习-tool-learning" class="headerlink" title="LLM用于工具学习(tool learning)"></a>LLM用于工具学习(tool learning)</h3><p>工具学习将专门工具与基础模型相结合，从而解决复杂问题。</p>
<p>LLM与外部工具（如数据库、搜索引擎等）相结合中，LLM常用作控制器，选择管理现有的模型解决问题。</p>
<p>LLM作为推荐代理时，也可以使用外部工具改善其缺少私有数据，或过时信息造成的问题。可用的工具有：搜索引擎，数据库，recommendation engine等。</p>
<h3 id="LLM作为个性化内容创建者"><a href="#LLM作为个性化内容创建者" class="headerlink" title="LLM作为个性化内容创建者"></a>LLM作为个性化内容创建者</h3><p>使用LLM针对不同个体生成个性化内容，如视频，广告等。存在的问题：真实性，数据隐私。</p>
<h3 id="仍然存在的问题"><a href="#仍然存在的问题" class="headerlink" title="仍然存在的问题"></a>仍然存在的问题</h3><p>计算能力不足，响应时间长；</p>
<p>个性化任务需要特定的数据集，需要针对数据集做调整；</p>
<p>长文本的窗口限制；</p>
<p>解释性问题，不够透明，可能涉及公平与道德问题</p>
<p>个性化系统的评价指标问题：针对大模型可能需要新增评价指标，如用户偏好，满意度等；</p>
<p>个性化系统需要在性能、准确度之间做权衡。</p>
<h2 id="LLM的一种个性化方法"><a href="#LLM的一种个性化方法" class="headerlink" title="LLM的一种个性化方法"></a>LLM的一种个性化方法</h2><p>文章名：Teach LLMs to Personalize – An Approach inspired by Writing Education</p>
<p>google实现</p>
<p>实现了一种个性化的文本生成的通用方法。</p>
<p>具体而言：多阶段多任务框架生成；检索-排序-总结-综合-生成。多任务：LLM应更好地阅读给定文本并理解其来源，才能生成更个性化的内容。</p>
<ul>
<li>研究发现写作生成任务的效果与阅读理解任务相关。因此，模型还需要判断给定的一堆文档是否由同一个作者撰写。</li>
</ul>
]]></content>
      <categories>
        <category>LLM application</category>
      </categories>
  </entry>
  <entry>
    <title>多模态文字处理有关模型介绍分析</title>
    <url>/Essay-analysis/multimodal-doc-comprehension/</url>
    <content><![CDATA[<p>以下是对一部分多模态文档检索领域论文的模型分析，包括Donut、TextMonkey、DocPedia、mPLUG系列。</p>
<span id="more"></span>

<h2 id="Donut"><a href="#Donut" class="headerlink" title="Donut"></a>Donut</h2><p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722095404407.png" alt="image-20240722095404407"></p>
<p>将图片通过encoder进行编码，得到其hidden state；再将hidden state和prompt一同输入decoder，得到最终输出。</p>
<h2 id="DocPedia"><a href="#DocPedia" class="headerlink" title="DocPedia"></a>DocPedia</h2><p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722113746394.png" alt="image-20240722113746394"></p>
<p>DocPedia的核心在于使用DCT变换，使得图像特征不受分辨率限制。通过这种方式，图像最大可以为2560*2560分辨率。</p>
<h2 id="TextMonkey"><a href="#TextMonkey" class="headerlink" title="TextMonkey"></a>TextMonkey</h2><p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722160230034.png" alt="image-20240722160230034"></p>
<p>TextMonkey在Monkey的基础上使用了shift window与token resampler方法，前者可以计算不同patch之间的attention score，后者可以将大量冗余的token进行合并，降低计算复杂度。</p>
<h2 id="mPLUG系列"><a href="#mPLUG系列" class="headerlink" title="mPLUG系列"></a>mPLUG系列</h2><p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722161828192.png" alt="image-20240722161828192"></p>
<p>mPLUG是系列模型的起源，该模型可以将图像和文本两种信息输入并进行融合。模型提出了skip connection的方法来跳过冗余视频特征，从而搭建了一个图文预训练模型。这个模型可以解决多种多模态任务。</p>
<p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722174104764.png" alt="image-20240722174104764"></p>
<p>mPLUG-2模型是模块化的模型，不同的任务使用不同的输入输出头，但内部的处理单元是统一的。这样就实现了模块化的大一统模型，在一定程度上解决了模态拉扯的问题。</p>
<p><img src="/Essay-analysis/multimodal-doc-comprehension/image-20240722175820988.png" alt="image-20240722175820988"></p>
<p>mPLUG-Owl是多模态对话大模型，将预训练的LLM和visual encoder结合，从而使用预训练参数降低训练开销。</p>
<ul>
<li>第一阶段对图文预训练，对齐文本图像空间：将LLM参数frozen，训练visual abstractor和visual encoder。其中visual abstractor和TextMonkey中的token resampler想法类似，将大量patch的冗余信息删除，得到更加详细的token；</li>
<li>第二阶段使用LoRA对LLM做instruction tuning，将visual abstractor和visual encoder参数frozen。</li>
</ul>
<p>mPLUG-Owl2使用了mPLUG-2的模态协同思想，同时提升文本和多模态任务的性能。（部分解决了模态拉车的问题，不会在提升多模态能力时导致LLM本身理解能力降低）</p>
<p>mPLUG-DocOwl针对文档理解做了优化。</p>
<ul>
<li>对于高分辨率的不规则大小图片，使用“形状自适应切图”，将原图裁剪为适配visual encoder的子图；</li>
<li>对子图进行编码，位置信息通过LLM语言端进行理解。</li>
</ul>
<p>文档使用abstractor可能导致空间位置信息丢失，因此将abstractor换为H-Reducer，其在时间上做卷积，降低token数同时保留空间信息。</p>
]]></content>
      <categories>
        <category>Essay analysis</category>
      </categories>
  </entry>
  <entry>
    <title>多模态有关论文模型介绍分析</title>
    <url>/Essay-analysis/multimodality-survey/</url>
    <content><![CDATA[<p>以下是对一部分多模态相关论文的模型分析，包括ViT、CLIP、Swin Transformer、VLMo、BLIP、LLaVa。</p>
<span id="more"></span>

<h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710103323153.png" alt="image-20240710103323153"></p>
<p>ViT的motivation是将Transformer应用到CV领域。这个想法的难点在于如何将NLP中token的embedding对应到图像中。</p>
<p>ViT对这个问题的解决方案是将图像划分为patches，例如将一张224x224x3的图像按长宽进行划分；假设每个patch的大小为14x14x3，则可以分成(224&#x2F;14)^2&#x3D;256个patch，并横向排成一个sequence，作为Transformer的输入。</p>
<p>上述操作可以得到图像的embedding，但存在的问题是：attention机制计算patches之间的attention score时，没有patches之间位置的信息。attention机制对所有图像块两两计算分数，因此两个patch交换顺序后仍然可以计算。因此，需要增加patch在图像中位置的编码。例如图像被分为9个patches，则给这9个patch编号1-9，并进行embedding，加到patch的embedding中，作为encoder的输入。</p>
<p>除此之外，为了匹配NLP任务中存在的[CLS]符号，在embedding输入encoder之前，还需要在最开始增加编号0的[CLS]embedding。</p>
<h2 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710104757859.png" alt="image-20240710104757859"></p>
<p>CLIP的motivation是使用Transformer做图文匹配。为此，他需要大量的图片文本对作为训练集。</p>
<p>CLIP的结构是将文本通过text encoder转换为embedding，将图片通过image encoder转换为embedding，最后计算这些embedding的相似度。</p>
<p>对于N个图片文本对，图片和文本的embedding可以构成两个张量：文本张量中$T_1,…,T_N$表示文本embedding，图片张量中$I_1,…,I_N$表示图片embedding。这两个张量中的embedding两两做内积，可以得到一个矩阵，表示相似度。</p>
<p>在这个相似度矩阵中，最理想的结果是对角线上元素为1，其他地方元素为0.计算交叉熵损失就可以得到差距，进而训练模型。</p>
<h2 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710111308736.png" alt="image-20240710111308736"></p>
<p>Swin Transformer的motivation是在ViT的基础上增强跨窗口能力。这可以通过移动窗口机制实现。</p>
<ul>
<li>Swin Transformer首先将图片划分为4x4的patch，将$H\times W\times 3$的图片划分为$\frac H 4\times\frac W 4\times (3\times 4\times 4)&#x3D;48$的张量；</li>
<li>C为超参数，上述张量通过线性映射到$\frac H 4\times\frac W 4\times C$的维度上；</li>
<li>Swin Transformer Block不改变维度，进入第二个Block维度仍然为$\frac H 4\times\frac W 4\times C$；</li>
<li>Patch Merging操作例：原来4*4矩阵，从上到下从左到右分别为1-16。现在将1 3 9 11合并为2*2矩阵，2 4 10 12合并，类似得到4个2*2矩阵<br>在channel上拼接（池化），得到$\frac H 2\times \frac W 2\times 4C$的张量，在经过1*1卷积将4C降为2C</li>
</ul>
<p>移动窗口多头自注意力：</p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240710114049122.png" alt="image-20240710114049122"></p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240710114027734.png" alt="image-20240710114027734"></p>
<p>每次移动原窗口的一半，使得不同窗口之间可以进行信息交互。但移动窗口后，4个窗口会变成9个窗口，增加计算量。可以使用循环移位方法维持4个窗口不变，但增加掩码防止原本不相邻的位置计算注意力分数。</p>
<h2 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710144932751.png" alt="image-20240710144932751"></p>
<p>MoCo的motivation是使用对比学习的方式做自监督学习。对比学习可以理解为检索字典的任务，根据query的embedding在字典中查找正样本的key对应的embedding，这两个embedding应该相似；与负样本的key对应的embedding应该不相似。</p>
<p>对比学习中，字典大小决定了训练效果；字典应该越大越好。但字典过大会导致每次更新权重的计算量增加，很可能无法训练。为解决该问题，文章引入了队列，每次从队列中抽取一部分key计算相似度，更新encoder权重后计算embedding加入队列尾，从队列头取出元素计算loss。</p>
<p>但这样会引入另一个问题：队列中每个元素的embedding都是由不同状态的encoder计算得到的，这会导致不一致性。为解决该问题，文章引入了动量机制（momentum）更新权重；即：每次对于key更新权重都保留大量原始信息，只做很小的调整。即：<br>$$<br>\theta_k \leftarrow m\theta_k+(1-m)\theta_q<br>$$<br>其中$m$取值常为0.99。这样就保证了query encoder更新时，key encoder更新更缓慢，保持了队列元素的一致性。</p>
<h2 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710153012398.png" alt="image-20240710153012398"></p>
<p>DETR的motivation是用Transformer做目标检测。在DETR之前的目标检测模型大多使用了非极大值抑制（non-maximum supression, nms）过程，导致模型复杂，超参数难调节。这篇文章没有使用nms过程，但仍然达到了较好的效果。</p>
<p>对于一张图片，DETR先生成N个框，再使用二分图匹配的匈牙利算法找到匹配最好的几个框，从而计算loss。具体而言，DETR的backbone模型是CNN（ResNet），抽取出图片特征后增加positional encoding，输入transformer encoder+decoder+FFN中生成预测的N个框，包括位置以及类别。在decoder阶段，transformer的输入是object queries，是可学习参数，用来引导生成目标框。</p>
<h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710160707171.png" alt="image-20240710160707171"></p>
<p>ALBEF是另一种学习图片与文本对齐的模型，与CLIP目的一致。</p>
<p>一般而言，图像比文本包含更多的信息，需要更深的网络提取特征。因此为了保持模型的规模与之前的模型可比，文章将文本encoder分为两部分，只用6层attention block作为text encoder，而图像的encoder仍然保持12层。</p>
<p>在12层的image encoder和6层的text encoder后，输入文本图像对的特征就已经被抽取出来了。每个文本图像对计算loss，就可以得到ITC loss。这个loss是在fusion之前计算的，即没有经过最后多模态encoder的计算就可以得到，因此这个loss就让image和text在fusion之前就进行了align，即标题。</p>
<p>ITM loss是经过fusion之后，在模型最后增加FC层，判断图像和文本是否在一个类这样一个二分类任物的loss。但实际上这个loss过于简单，因为负样本数量远远大于正样本，直接输出不属于一个类别准确率也很高。因此可以通过ITC loss得到最接近正样本的负样本（hard negatives），将这个负样本作为计算loss的根据，使得这个loss更加challenging。</p>
<p>MLM loss即BERT中对语句进行MASK操作后判断MASK掉的单词，从而计算loss。这次输入的text是被MASK的语句，但其他两个loss都是原始text，因此需要两次forward过程。</p>
<p>Momentum Distillation：因为数据noise很大，可能生成的负样本描述对图片也是一个很好的描述。因此需要构建一个momentum model，使用这个模型生成pseudo targets，使得one hot label变为multi-hot label。这样在训练时，如果原始label是错误的，或是noisy的时候，momentum label也可以提供帮助。这两个模型生成两个loss，通过momentum方式影响最终loss即可。</p>
<h2 id="VLMo"><a href="#VLMo" class="headerlink" title="VLMo"></a>VLMo</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710174920050.png" alt="image-20240710174920050"></p>
<p>VLMo的motivation是发现：</p>
<ul>
<li>以CLIP为首的双塔（dual-encoder）架构因为计算模态间交互只使用了矩阵乘法，在检索上很高效；但这种简单的交互面对复杂的情况难以建模；</li>
<li>单塔架构：仅对文本和图像进行相对简单的预处理得到embedding，在模态间交互时使用encoder进行建模。这种方式建模能力强，但推理慢（因为要通过encoder）。</li>
</ul>
<p>因此将这两种结构进行结合，根据不同情景调用不同expert即可。（共享参数）</p>
<h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710193309555.png" alt="image-20240710193309555"></p>
<p>BLIP的motivation有两个角度：</p>
<ul>
<li>模型角度：encoder only的模型可以进行图像文本retrieval，但不能直接用来进行文本生成（例如图像生成字幕）；encoder+decoder的模型可以进行文本生成，但还未用到图像文本retrieval。如果使用一个模型将这些功能进行unify，将会提升功能。</li>
<li>数据角度：无论脏数据集的大小，其都会在一定程度上影响训练结果。</li>
</ul>
<p>因此BLIP提出：可以在不同的应用上使用不同的模型，共享部分参数。结合了ALBEF和VLMo的特点。</p>
<p>图像仍然使用标准ViT模型，针对文本有3个不同encoder&#x2F;decoder，包括：</p>
<ul>
<li>对文本的text encoder，生成text embedding，可以用来和图像的embedding计算itc分数；</li>
<li>加上了图像信息的image-grounded text encoder，可以直接用来生成图文匹配结果（二分类）；</li>
<li>类似GPT的生成模型image-grounded text decoder，可以借助图像和（初始）文本信息，生成符合图像信息的文本。</li>
</ul>
<p>这三个文本模型中，前两个模型的self attention模块共享权重；后两个模型的cross attention模块共享权重；三个模型的feed forward模块共享权重。</p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240710194140782.png" alt="image-20240710194140782"></p>
<p><strong>Captioning and Filtering</strong>：已知目前有大量的noisy数据集，该方法提供了一种消除噪音训练的方法。noisy数据集为${(I_w,T_w)}$图像文本对，其中$T_w$不可信；人工标注数据集为${(I_h,T_h)}$图像文本对，其中$T_h$可信。</p>
<ul>
<li>首先使用这两种数据集训练，得到一个初步可用的网络；</li>
<li>将其中的image-grounded text encoder（即第一张图的第二个text模型）提出，作为filter；将image-grounded text-decoder（即第一张图的第三个text模型）作为captioner；</li>
<li>对于noisy数据集中的所有$(I_w,T_w)$数据对，都做以下操作：<ul>
<li>使用filter对$(I_w,T_w)$做检验，如果不通过filter就删除；</li>
<li>使用captioner对$I_w$生成caption，成为新的$(I_w,T_w)$数据对，再经过filter检验；</li>
<li>只有通过filter的数据对才能保留。</li>
</ul>
</li>
<li>通过上述方式，留下的数据对包括：<ul>
<li>通过了检验的noisy数据集原始数据对$(I_w,T_w)$；</li>
<li>通过了检验的noisy数据集图片和captioner生成的caption数据对$(I_w,T_s)$；</li>
<li>人工标注的数据集原始数据对$(I_h,T_h)$。</li>
</ul>
</li>
<li>留下的数据对质量更高，可以用来重新训练模型。</li>
</ul>
<h2 id="BLIP2"><a href="#BLIP2" class="headerlink" title="BLIP2"></a>BLIP2</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240710210918306.png" alt="image-20240710210918306"></p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240710210617787.png" alt="image-20240710210617787"></p>
<p>BLIP2的motivation是使用LLM作为模型的一部分时，如果冻结LLM参数，只训练其他部分，训练效率应该会提高。</p>
<p>不更新LLM参数代表着第一张图中image encoder和LLM冻结，但直接连接二者会导致特征空间不匹配，因此需要增加一个适配层，即Querying Transformer。训练Q-Former的过程仍然使用ITM等loss。</p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240710211657315.png" alt="image-20240710211657315"></p>
<p>在生成任务中，因为Q-Former已经提取了图像的特征并转化为了语言模型的表达形式，因此通过一个全连接层就可以直接输入到预训练的encoder&#x2F;decoder中，用于生成描述。生成描述既可以通过直接将embedding注入decoder来生成，也可以通过将embedding和实际文本的embedding结合，通过encoder+decoder结构来生成。</p>
<h2 id="LLaVA-LLaVA-1-5"><a href="#LLaVA-LLaVA-1-5" class="headerlink" title="LLaVA&#x2F;LLaVA-1.5"></a>LLaVA&#x2F;LLaVA-1.5</h2><p><img src="/Essay-analysis/multimodality-survey/image-20240711145255124.png" alt="image-20240711145255124"></p>
<p>LLaVA关注点在于将image信息与LLM的文本输入信息对齐。图像在通过vision encoder得到其hidden state后，通过一个projection layer将其转化为text的hidden state，进而将image和text信息进行统一。</p>
<p>在文本处理中，如果涉及到图像信息，则将文本中的&lt;image&gt; token替换为图像的embedding即可。</p>
<p><img src="/Essay-analysis/multimodality-survey/image-20240711174441702.png" alt="image-20240711174441702"></p>
<p>LLaVA-1.5使用了多种优化方式，例如更换更大的LLM、增加分辨率等，实现了更高的准确率。</p>
]]></content>
      <categories>
        <category>Essay analysis</category>
      </categories>
  </entry>
  <entry>
    <title>推荐系统综述 文章分析</title>
    <url>/Essay-analysis/recommendation-system-survey/</url>
    <content><![CDATA[<p>文章地址：<a href="https://arxiv.org/abs/2104.13030">https://arxiv.org/abs/2104.13030</a></p>
<p>文章名：<strong>A Survey on Accuracy-oriented Neural Recommendation: From Collaborative Filtering to Information-rich Recommendation</strong></p>
<p>面向准确性的神经推荐综述：从协同过滤到信息丰富推荐</p>
<span id="more"></span>
<h2 id="Abstract-Conclusion"><a href="#Abstract-Conclusion" class="headerlink" title="Abstract &amp; Conclusion"></a>Abstract &amp; Conclusion</h2><p>深度学习对推荐模型产生较大影响；</p>
<p>本文将基于深度学习的推荐系统领域的工作分为：</p>
<ul>
<li>协同过滤(collaborative filtering)</li>
<li>内容丰富的推荐(content enriched recommendation)</li>
<li>时间顺序推荐(temporal&#x2F;sequential recommendation)</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>推荐系统的研究可追溯到1990，基于内容的启发式和协同过滤；矩阵分解模型（2008~2016）；效率低；</li>
<li>2010中期，深度学习：从具有复杂模式的大数据中学习特别有利；推荐系统也出现了很多基于神经网络的工作，即神经推荐模型(neural recommender models)；</li>
</ul>
<h3 id="与其他综述的不同"><a href="#与其他综述的不同" class="headerlink" title="与其他综述的不同"></a>与其他综述的不同</h3><ul>
<li>现有综述大多聚焦于特定主题，或遵循深度学习的分类方式；</li>
<li>本综述将着眼于准确性。</li>
</ul>
<h3 id="组织结构"><a href="#组织结构" class="headerlink" title="组织结构"></a>组织结构</h3><ul>
<li><p>学习推荐的问题可抽象为：<br>$$<br>\hat{y}_{u,i,c}&#x3D;f(D_u,D_i,D_c)<br>$$<br>给定用户$u$、物品$i$、上下文$c$的数据$D_u,D_i,D_c$，让函数$f$学会预测可能性$y$。</p>
</li>
<li><p>不同模型的特点：</p>
<ul>
<li>协同过滤模型：忽略$D_c$，只使用用户和物品的ID和<strong>交互</strong>历史数据；</li>
<li>将用户和物品的辅助信息基层到推荐系统的模型（内容丰富模型）：加入辅助信息（用户和物品信息），忽略$D_c$；</li>
<li>使用上下文数据的模型（context-aware model）：三种数据全部考虑；关注最常见的时间上下文（temporal context）；</li>
</ul>
</li>
<li><p>可将上下文信息视为用户数据的一部分；</p>
</li>
</ul>
<h2 id="Collaborative-Filtering-Models-CF"><a href="#Collaborative-Filtering-Models-CF" class="headerlink" title="Collaborative Filtering Models (CF)"></a>Collaborative Filtering Models (CF)</h2><p>利用所有用户的协作行为预测目标用户的行为</p>
<p>使用基于记忆的模型 -&gt; 基于矩阵分解的模型</p>
<p>当前：</p>
<ul>
<li>用户和物品的表示建模；</li>
<li>给定表示，用户和物品交互建模</li>
</ul>
<h3 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h3><p>目标：学习用户嵌入矩阵P和物品嵌入矩阵Q</p>
<p>难点：用户-物品交互行为的稀疏性。</p>
<h4 id="历史行为聚合增强模型-history-behavior-attention-aggregation-models"><a href="#历史行为聚合增强模型-history-behavior-attention-aggregation-models" class="headerlink" title="历史行为聚合增强模型(history behavior attention aggregation models)"></a>历史行为聚合增强模型(history behavior attention aggregation models)</h4><p>传统方法使用独热码关联用户和物品；增加历史信息以达到更好地建模。仍然使用线性矩阵分解。例：FISM model、SVD++</p>
<p>ACF增加神经注意机制：为交互过的物品分配权重，表明对用户表示的重要性；</p>
<p><img src="/Essay-analysis/recommendation-system-survey/image-20230502215120856.png" alt="image-20230502215120856"></p>
<p>$\alpha(u,j)$：用户与物品间注意力权重</p>
<p>NAIS删除用户项，便于对不同用户同时建模；</p>
<p><img src="/Essay-analysis/recommendation-system-survey/image-20230502215659301.png" alt="image-20230502215659301"></p>
<p>$\alpha(i,j)$：两件物品间注意力权重</p>
<p>应用例：DeepICF、DIN</p>
<h4 id="基于自动编码器的模型-autoencoder-based-representation-learning"><a href="#基于自动编码器的模型-autoencoder-based-representation-learning" class="headerlink" title="基于自动编码器的模型(autoencoder based representation learning)"></a>基于自动编码器的模型(autoencoder based representation learning)</h4><p>基本路径：输入不完整的用户物品矩阵 -&gt; 编码器学习隐藏表示 -&gt; 解码器重建输入</p>
<p>方式：</p>
<ul>
<li>输入每个用户的历史记录学习用户偏好；</li>
<li>输入用户对物品的评分学习用户偏好；</li>
</ul>
<p>模型扩展：</p>
<ul>
<li>使用自动编码器变体（使用深度学习技术）；</li>
<li>设计两个并行编码器学习用户和项目表示，使用内积模拟偏好；</li>
</ul>
<p><strong>聚合历史行为，可视为基于历史行为的扩展</strong></p>
<h4 id="图学习方法-graph-learning-approaches"><a href="#图学习方法-graph-learning-approaches" class="headerlink" title="图学习方法(graph learning approaches)"></a>图学习方法(graph learning approaches)</h4><p>使用图神经网络对图数据结构建模。</p>
<p>例：SpectralCF、GC-MC、NGCF</p>
<ul>
<li>基于神经图的CF模型和经典GNN不同，因此提出新模型LR-GCCF&#x2F;LightGCN等消除不必要的深度学习步骤；简化的模型效果更好。</li>
</ul>
<h3 id="交互建模"><a href="#交互建模" class="headerlink" title="交互建模"></a>交互建模</h3><h4 id="基于内积的指标"><a href="#基于内积的指标" class="headerlink" title="基于内积的指标"></a>基于内积的指标</h4><p>以用户embedding和物品embedding的内积估计偏好；</p>
<p>主要限制：</p>
<ul>
<li>违反三角不等式；缺乏用户之间、物品之间的关联；</li>
<li>线性交互建模，无法得到复杂关系</li>
</ul>
<h4 id="基于距离的指标"><a href="#基于距离的指标" class="headerlink" title="基于距离的指标"></a>基于距离的指标</h4><p>使用距离度量作为交互函数；</p>
<p>CML使用欧几里得空间的距离；使用翻译原理模拟行为；</p>
<p>LRML引入关系向量（可训练），效果更好。</p>
<h4 id="基于神经网络的指标"><a href="#基于神经网络的指标" class="headerlink" title="基于神经网络的指标"></a>基于神经网络的指标</h4><p>使用MLP、CNN等作为构建块，挖掘交互的复杂和非线性模式；</p>
<p>NCF使用MLP对每个用户-物品对建模；加入MF组件提高推荐质量。</p>
<p>基于CNN：侧重高阶相关性，增加复杂度和成本。</p>
<p>使用AE填补交互矩阵的空白；</p>
<h3 id="GNN优越性"><a href="#GNN优越性" class="headerlink" title="GNN优越性"></a>GNN优越性</h3><ul>
<li>数据结构：用户-物品交互可抽象为二分图；</li>
<li>可显式编码用户-物品交互的关键协同过滤信号（？）</li>
</ul>
<h2 id="Content-Enriched-Recommendation"><a href="#Content-Enriched-Recommendation" class="headerlink" title="Content-Enriched Recommendation"></a>Content-Enriched Recommendation</h2><p>CF中仅对用户的行为模式进行了编码，而CER使用辅助数据；</p>
<p>辅助数据分为基于内容的信息和上下文感知数据：</p>
<p>内容信息与用户和物品有关；上下文信息显示了用户决策的环境，如时间数据等；</p>
<h3 id="用户和项目的一般特征"><a href="#用户和项目的一般特征" class="headerlink" title="用户和项目的一般特征"></a>用户和项目的一般特征</h3><p>FM(Factorization Machine)：见<a href="https://zhuanlan.zhihu.com/p/109541098">FM</a></p>
<p>FM优点：对稀疏的数据同样有效，并减少参数大小；</p>
<h4 id="基于MLP（多层感知机）的高阶建模"><a href="#基于MLP（多层感知机）的高阶建模" class="headerlink" title="基于MLP（多层感知机）的高阶建模"></a>基于MLP（多层感知机）的高阶建模</h4><p>使用MLP发现高阶相关性；由于黑盒的存在，需要预训练或结构优化。</p>
<p>DeepCrossing：加入残差结构；</p>
<p>NFM：双交互操作；</p>
<p>PNN：按bit的交互建模；</p>
<h4 id="用于K阶建模的交叉网络"><a href="#用于K阶建模的交叉网络" class="headerlink" title="用于K阶建模的交叉网络"></a>用于K阶建模的交叉网络</h4><p>设计网络时就设计了交叉网络，在向量级别进行交叉交互；</p>
<p>$x_k&#x3D;x_0x_{k-1}w_k+b_k+x_{k-1}$</p>
<h4 id="树增强建模"><a href="#树增强建模" class="headerlink" title="树增强建模"></a>树增强建模</h4><p>使用树来展现交叉特征；</p>
<p>TEM：决策树提取高阶交互特征，将得到的交互特征输入到注意力模型中。</p>
<h3 id="文本内容信息"><a href="#文本内容信息" class="headerlink" title="文本内容信息"></a>文本内容信息</h3><p>基于神经网络的NLP模型，进行文本增强的推荐。</p>
<p>内容输入可分为：与项目或用户有关的内容描述；用户-项目对。第二类在多数情况下退化为第一类。</p>
<h4 id="基于自动编码器-AE-的模型"><a href="#基于自动编码器-AE-的模型" class="headerlink" title="基于自动编码器(AE)的模型"></a>基于自动编码器(AE)的模型</h4><p>使用自动编码器（或变体）进行隐藏内容的提取。</p>
<p>CDL(Collaborative Deep Learning)：使用堆叠去噪AE学习物品内容的隐藏表示，优化过程通过学习压缩表示和重建过程捕捉内容信息；</p>
<p>$q_i&#x3D;f_e(x_i)+\theta_i$</p>
<h4 id="词嵌入模型"><a href="#词嵌入模型" class="headerlink" title="词嵌入模型"></a>词嵌入模型</h4><p>AE没有考虑到文本输入的独一性，因此引入词嵌入模型。</p>
<p>ConvMF：结合了卷积神经网络和概率矩阵分解，通过TextCNN模型进行学习。</p>
<p>基于评论的深度推荐模型：如DeepCoNN使用并行的TextCNN模型分别学习用户的评论和物品的评论，再使用分解机学习交互。</p>
<p>目标评论信息不可用时：TransNet。</p>
<h4 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h4><p>给不同的内容片段分配不同权重，从而自动选择出信息丰富的关键词进行推荐。</p>
<h4 id="文本解释"><a href="#文本解释" class="headerlink" title="文本解释"></a>文本解释</h4><p>可以为推荐系统提供文本解释。</p>
<p>基于提取的文本解释：可通过注意力机制，选择具有较大注意力权重的文本片段作为解释的内容。</p>
<p>基于生成的文本解释：通过编码器和解码器机制，输入用户与物品的信息，并输出评论。使用了RNN和MLP技术。</p>
<h3 id="多媒体信息"><a href="#多媒体信息" class="headerlink" title="多媒体信息"></a>多媒体信息</h3><p>多媒体信息包括图片、视频、音频等。</p>
<h4 id="图片推荐"><a href="#图片推荐" class="headerlink" title="图片推荐"></a>图片推荐</h4><p>可分为两类：基于内容的模型、混合推荐模型。</p>
<ol>
<li><p>基于内容的模型：适用于依赖视觉影响的情况。使用CNN提取视觉信息，并与文本结合。</p>
</li>
<li><p>混合推荐模型：同时使用协同信号和视觉内容进行推荐，可以提高性能。</p>
<p>VBPR：使用视觉内容进行统一混合推荐：在两个空间内投影，并结合两个空间内的用户偏好学习预测偏好。</p>
<p>在VBPR基础上也有进一步拓展，如时间演化、图像关联位置表示等。</p>
<p>此外还有GNN方法。</p>
</li>
</ol>
<h4 id="视频推荐"><a href="#视频推荐" class="headerlink" title="视频推荐"></a>视频推荐</h4><p>提取视频、音频特征后通过神经网络进行融合。</p>
<p>ACF模型：可总结视频历史记录部分的偏好。</p>
<h3 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a>社交网络</h3><p>社交网络中存在社交关系，兴趣存在相关性。可分为两类：社会关联增强和正则化模型，以及基于GNN的模型。</p>
<h4 id="社会关联增强和正则化模型"><a href="#社会关联增强和正则化模型" class="headerlink" title="社会关联增强和正则化模型"></a>社会关联增强和正则化模型</h4><p>将用户行为视为社交领域，将物品偏好行为视为物品领域，并将两种行为融合起来。</p>
<p>可以通过RNN、注意力模型等进行建模。</p>
<h4 id="基于GNN的模型"><a href="#基于GNN的模型" class="headerlink" title="基于GNN的模型"></a>基于GNN的模型</h4><p>现实世界中社会扩散过程是递归的，即每个用户都受到全局图结构的递归影响。因此使用GNN模型提高模拟效果。例：DiffNet；</p>
<p>可以使用异构GNN进行建模。例：DiffNet++，效果最好。</p>
<h3 id="知识图谱-KG"><a href="#知识图谱-KG" class="headerlink" title="知识图谱(KG)"></a>知识图谱(KG)</h3><p>知识图谱提供了用户与物品之间的侧面信息。可分为三类：基于路径的模型、基于正则化的模型、基于GNN的模型。</p>
<h4 id="基于路径的模型"><a href="#基于路径的模型" class="headerlink" title="基于路径的模型"></a>基于路径的模型</h4><p>用户到物品的路径可以通过连续的序列表示，通过描述该路径即可得到偏好。但由于获得路径较为复杂且规模较大，难以应用。</p>
<h4 id="基于正则化的模型"><a href="#基于正则化的模型" class="headerlink" title="基于正则化的模型"></a>基于正则化的模型</h4><p>知识图谱可以作为额外的损失项用于正则化模型的学习。例：CKE</p>
<h4 id="基于GNN的模型-1"><a href="#基于GNN的模型-1" class="headerlink" title="基于GNN的模型"></a>基于GNN的模型</h4><p>通过GNN确保高阶连通性。例：KGAT</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>可以通过增加文本、多媒体、社交网络等辅助数据来提升推荐性能，关键是辅助数据的选择和集成方法。</p>
<p>使用注意力机制可以筛选出最相关的信息；使用GNN方法可以得到更好的结构信息以及高阶相关性。</p>
<h2 id="Temporal-Sequential-Models"><a href="#Temporal-Sequential-Models" class="headerlink" title="Temporal &amp; Sequential Models"></a>Temporal &amp; Sequential Models</h2><p>用户的偏好是随时间变化的，该类模型就是要找到用户的动态偏好或偏好随时间的变化。</p>
<p>可分为基于时间的推荐、基于会话的推荐、以及基于时间和会话的推荐。</p>
<h3 id="基于时间的推荐"><a href="#基于时间的推荐" class="headerlink" title="基于时间的推荐"></a>基于时间的推荐</h3><p>大多基于RNN实现。还可考虑上下文因素，如社交影响等来提高模型性能。</p>
<p>例：RRN（递归推荐网络）</p>
<p>最近研究增加了记忆网络等模块用于储存记忆，从而更好地捕捉用户的长期和短期兴趣。</p>
<h3 id="基于会话的推荐"><a href="#基于会话的推荐" class="headerlink" title="基于会话的推荐"></a>基于会话的推荐</h3><p>现实世界中常遇到匿名用户（用户ID不可用）的短会话数据，因此引入基于会话的推荐。</p>
<p>例：GRU4REC：类似RNN，递归地更新隐藏状态并输出物品。</p>
<p>同样的，存在基于注意力机制、GNN的模型。</p>
<h3 id="基于时间和会话的推荐"><a href="#基于时间和会话的推荐" class="headerlink" title="基于时间和会话的推荐"></a>基于时间和会话的推荐</h3><p>该类模型同时使用时间和会话进行学习。</p>
<p>可分为两类：第一类学习用户的长期偏好和短期动态偏好，第二类学习统一的用户表示。</p>
<ol>
<li><p>长期&#x2F;短期偏好</p>
<p>从历史行为得到长期偏好，从最近的会话得到短期偏好。可以使用分层的注意力网络进行建模。</p>
</li>
<li><p>统一用户表示</p>
<p>使用三维神经网络进行推荐。例：Caser</p>
</li>
</ol>
<p>同样存在使用GNN的模型进行推荐，且表现出色。</p>
<h2 id="Discussion-and-Future-Directions"><a href="#Discussion-and-Future-Directions" class="headerlink" title="Discussion and Future Directions"></a>Discussion and Future Directions</h2><p>仍有以下方面可以进行优化：</p>
<ol>
<li>推荐的Benchmark。推荐方式和场景多种多样，如何为各种推荐方式指定一套统一的Benchmark十分重要。</li>
<li>图推理技术与自监督学习。图上深度学习十分成功，但自然的图推荐技术尚可改进；将自监督学习整合到推荐中也可解决数据稀疏性等问题。</li>
<li>多目标社会公益推荐。以前的推荐系统大多只关注推荐准确性，但忽略了社会方面的问题，可能造成偏差。</li>
<li>复现性。神经网络对初始化、调参等许多方面的依赖程度较高且十分敏感，进行性能测试十分困难。</li>
</ol>
]]></content>
      <categories>
        <category>Essay analysis</category>
      </categories>
  </entry>
  <entry>
    <title>Windows安装miniconda和pytorch环境</title>
    <url>/Windows/windows-conda-pytorch-installation/</url>
    <content><![CDATA[<p>此处记录windows环境下miniconda以及pytorch环境的建立过程以及需要注意的事项。</p>
<span id="more"></span>

<h2 id="conda环境安装与配置"><a href="#conda环境安装与配置" class="headerlink" title="conda环境安装与配置"></a>conda环境安装与配置</h2><blockquote>
<p>选择miniconda是因为相较于anaconda，miniconda体积较小，删去了很多用不上的功能。</p>
</blockquote>
<p>下载地址：<a href="https://docs.conda.io/en/latest/miniconda.html">https://docs.conda.io/en/latest/miniconda.html</a></p>
<p>在windows环境下，选择windows平台下的64位安装包，即下图中的第一个选项。</p>
<p><img src="/Windows/windows-conda-pytorch-installation/image-20230710193210962.png" alt="image-20230710193210962"></p>
<blockquote>
<p>linux环境下则要按照指令集选择对应的版本下载安装包。</p>
</blockquote>
<p>按照流程安装即可。</p>
<blockquote>
<p>按照默认设置将不会把conda加入环境变量中。如果想要手动加入环境变量，以下为环境变量列表：</p>
<p>（<code>D:\miniconda3</code>是我的miniconda安装路径，加入环境变量时注意修改为自己的路径）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">D:\miniconda3</span><br><span class="line">D:\miniconda3\Library\mingw-w64\bin</span><br><span class="line">D:\miniconda3\Library\usr\bin</span><br><span class="line">D:\miniconda3\Library\bin</span><br><span class="line">D:\miniconda3\Scripts</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="conda虚拟环境搭建"><a href="#conda虚拟环境搭建" class="headerlink" title="conda虚拟环境搭建"></a>conda虚拟环境搭建</h3><p>类似python的virtualenv，conda可以搭建虚拟环境以实现不同环境的隔离，从而允许不同版本的库在不同程序中正确运行。但conda虚拟环境与virtualenv不同的是，不同的虚拟环境中允许安装不同版本的python，更便于用户使用。</p>
<p>在不同的环境中，命令行的地址前方会显示当前的虚拟环境名称，如下图所示：</p>
<p><img src="/Windows/windows-conda-pytorch-installation/image-20230710194554448.png" alt="image-20230710194554448"></p>
<p>与虚拟环境相关的常用命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n <span class="string">&quot;虚拟环境的名称&quot;</span> python=3.x <span class="comment"># 新建虚拟环境</span></span><br><span class="line">conda <span class="built_in">env</span> list <span class="comment"># 查看虚拟环境列表</span></span><br><span class="line">conda activate <span class="string">&quot;虚拟环境的名称&quot;</span> <span class="comment"># 转到某个虚拟环境</span></span><br><span class="line">conda remove -n <span class="string">&quot;虚拟环境的名称&quot;</span> --all <span class="comment"># 删除某个虚拟环境以及内部的所有包</span></span><br></pre></td></tr></table></figure>

<h3 id="conda第三方库配置"><a href="#conda第三方库配置" class="headerlink" title="conda第三方库配置"></a>conda第三方库配置</h3><p>在conda虚拟环境内部，安装第三方库除了通过pip安装，还可以通过conda安装。</p>
<p>conda配置第三方库的命令如下：（注意要转到需要安装库的虚拟环境中）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install <span class="string">&quot;第三方库的名称&quot;</span> <span class="comment"># 安装第三方库</span></span><br><span class="line">conda update --all <span class="comment"># 更新该虚拟环境下的所有第三方库</span></span><br><span class="line">conda update <span class="string">&quot;第三方库的名称&quot;</span> <span class="comment"># 更新某一个库</span></span><br><span class="line">conda uninstall <span class="string">&quot;第三方库的名称&quot;</span> <span class="comment"># 卸载某一个库</span></span><br></pre></td></tr></table></figure>

<h3 id="conda换源"><a href="#conda换源" class="headerlink" title="conda换源"></a>conda换源</h3><p>此处使用清华源。</p>
<p>先执行<code>conda config --set show_channel_urls yes</code>命令，在用户目录下生成.condarc文件，再往该文件中写入：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">channels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">defaults</span></span><br><span class="line"><span class="attr">show_channel_urls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">default_channels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span></span><br><span class="line"><span class="attr">custom_channels:</span></span><br><span class="line">  <span class="attr">conda-forge:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">msys2:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">bioconda:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">menpo:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">pytorch:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">pytorch-lts:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">simpleitk:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></span><br><span class="line">  <span class="attr">deepmodeling:</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/</span></span><br></pre></td></tr></table></figure>

<p>即可将源换为清华源。</p>
<h2 id="pytorch安装与配置"><a href="#pytorch安装与配置" class="headerlink" title="pytorch安装与配置"></a>pytorch安装与配置</h2><p>前往pytorch的官网<a href="https://pytorch.org/">https://pytorch.org/</a>，找到选择安装平台与版本的选项栏，选择对应的版本进行安装。</p>
<p><img src="/Windows/windows-conda-pytorch-installation/image-20230710205605502.png" alt="image-20230710205605502"></p>
<p>选择使用conda环境安装，再选择cuda版本，即可得到对应的指令。运行指令即可安装完pytorch。</p>
<blockquote>
<p>低版本的pytorch可以在高版本的cuda上运行。但最好选择与cuda版本相适应的pytorch版本进行安装。</p>
</blockquote>
<p>安装完后，可以通过如下的指令检查gpu是否可用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span>	<span class="comment"># True表示cuda可用</span></span><br></pre></td></tr></table></figure>

<h2 id="jupyter-notebook安装与配置"><a href="#jupyter-notebook安装与配置" class="headerlink" title="jupyter notebook安装与配置"></a>jupyter notebook安装与配置</h2><p>通过以下指令来安装jupyter notebook。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install jupyter</span><br></pre></td></tr></table></figure>

<p>安装完成后，可通过命令行输入<code>jupyter notebook</code>来启动jupyter notebook。</p>
<blockquote>
<p>取消jupyter notebook密码：</p>
<ol>
<li>输入<code>jupyter notebook --generate-config</code>，生成一个py文件；</li>
<li>找到py文件中的<code>\#c.NotebookApp.token = &#39;&lt;generated&gt;&#39;</code>，改为<code>c.NotebookApp.token = &#39;&#39;</code>；</li>
<li>重启jupyter notebook即可。</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>Windows</category>
      </categories>
  </entry>
  <entry>
    <title>建站注意事项</title>
    <url>/Hexo/site-constuction-info/</url>
    <content><![CDATA[<h2 id="初衷"><a href="#初衷" class="headerlink" title="初衷"></a>初衷</h2><p>建立这个网站的初衷是为了记录自己大学生活中的一些想法、踩过的一些坑，并可以做一个树洞抒发一下情感。在这里记录一下建立这个网站的注意事项与一些资源路径。</p>
<p>该文章整体框架基于以下博客：</p>
<ul>
<li>15wylu <a href="https://blog.csdn.net/qq_32767041/category_8927471.html">Hexo NexT</a></li>
</ul>
<p>主要步骤均在上述博客中，以下只记录一些注意事项与想法。</p>
<span id="more"></span>

<h2 id="1-安装node-js-Hexo-NexT"><a href="#1-安装node-js-Hexo-NexT" class="headerlink" title="1. 安装node.js Hexo NexT"></a>1. 安装node.js Hexo NexT</h2><ul>
<li>windows下直接到官网下载<a href="https://nodejs.org/zh-cn/">node.js</a>与<a href="https://git-scm.com/">git</a>即可。</li>
<li>npm一定要改源(taobao)。</li>
<li><code>git clone</code>时不时抽风，clone不下来。不妨试试ladder。但ladder有时候也不好使。这时候就需要给电脑烧三根香，碰碰运气。运气好了就能clone也能push；运气不好clone一个小时也不行，push一晚也push不上去。</li>
</ul>
<h2 id="2-Hexo"><a href="#2-Hexo" class="headerlink" title="2. Hexo"></a>2. Hexo</h2><ul>
<li><p><code>hexo init</code>操作必须在空文件夹中进行。里面有无关文件也不行。</p>
</li>
<li><p>node_modules是真的nt。搞一个小网站需要这么多插件吗？</p>
</li>
<li><p>可以修改scaffolds文件夹中的模板。</p>
<p>例如建站中不想要tags但是想要categories，那模板中就可以增加categories，删去tags。</p>
</li>
<li><p>source文件夹中只有**_post**是文章文件夹。所有在首页出现的博客文章均位于_post中。不在_post中的文件夹以及其中的文件将不会在主页中显示，只能通过特定链接访问，因此可以放一些隐藏文件。此外about、categories、tags等捷径也在单独的文件夹中。</p>
</li>
<li><p><code>hexo new [] &quot;&quot;</code>有点烦人。想要新建文章，直接在文件夹里面新建还不行，需要单独开命令行，建完了再开typora。不知道有没有解决办法。</p>
</li>
<li><p>categories的标签方法为yaml语法。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="string">Study</span>, <span class="string">math</span>]</span><br><span class="line"><span class="comment"># - [new category]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>hexo deploy --debug</code>中注意debug前面是两个横线。</p>
</li>
<li><p>插入图片：使用hexo-asset-image插件。可参考<a href="https://blog.csdn.net/m0_43401436/article/details/107191688">hexo博客中插入图片失败——解决思路及个人最终解决办法</a></p>
</li>
<li><p>部署至Github、源代码上传至Github是两个事。部署是把自己生成的静态文件（如index.html、css配置文件、js脚本）上传到Github上；源代码上传就是源代码（source文件夹以及md文件、themes文件夹、config.yml等）上传。</p>
</li>
<li><p>数学公式：使用mathjax配置。参考：<a href="https://blog.csdn.net/u014630987/article/details/78670258">如何在 hexo 中支持 Mathjax？</a></p>
<p>基本步骤：</p>
<ol>
<li>修改渲染引擎至kramed（kram &lt;- mark倒序）</li>
<li>mathjax cdn刷新</li>
<li>更改转义规则</li>
<li>config.yaml开启mathjax</li>
</ol>
</li>
<li><p>ftpsync至home.ustc.edu.cn失败，卡在commiting处mkdirs后。解决办法：<a href="https://es2q.com/blog/2019/05/12/Hexo-ftpsync-bug/">Hexo ftpsync错误分析</a></p>
</li>
</ul>
<h2 id="3-NexT"><a href="#3-NexT" class="headerlink" title="3. NexT"></a>3. NexT</h2><ul>
<li>鼠标点击特效：参考<a href="https://jrbcode.gitee.io/posts/80095cae.html">在Hexo+NexT博客中设置鼠标点击特效</a></li>
<li>文章折叠：使用了<code>&lt;!-- more --&gt;</code>标志。有插件可以完成此工作，但是我并未使用。</li>
</ul>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>以上基本是重点注意事项了。建这个站，还是希望自己能多写写东西，多多记录一下生活。愿我能达到吧。</p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
</search>
